# ğŸš€ DQN LunarLander - KompletnÃ­ dokumentace

## ğŸ“‹ Obsah
1. [PÅ™ehled projektu](#pÅ™ehled-projektu)
2. [Architektura](#architektura)
3. [Soubory a jejich funkce](#soubory-a-jejich-funkce)
4. [Instalace](#instalace)
5. [PouÅ¾itÃ­](#pouÅ¾itÃ­)
6. [PokroÄilÃ© funkce](#pokroÄilÃ©-funkce)
7. [Hyperparametry](#hyperparametry)
8. [Troubleshooting](#troubleshooting)

---

## ğŸ“– PÅ™ehled projektu

Tento projekt implementuje **Double DQN s Dueling architekturou a Prioritized Experience Replay** pro Å™eÅ¡enÃ­ Ãºlohy **LunarLander-v3** z knihovny Gymnasium.

### ğŸ¯ CÃ­l
NatrÃ©novat agenta, kterÃ½ dokÃ¡Å¾e bezpeÄnÄ› pÅ™istÃ¡t lunÃ¡rnÃ­ modul mezi dvÄ›ma vlajkami s prÅ¯mÄ›rnÃ½m skÃ³re **200+**.

### ğŸ† KlÃ­ÄovÃ© featury
- âœ… **Double DQN** - sniÅ¾uje overestimation bias
- âœ… **Dueling architecture** - oddÄ›lenÃ© value a advantage streams
- âœ… **Prioritized Experience Replay (PER)** - efektivnÄ›jÅ¡Ã­ uÄenÃ­
- âœ… **AdaptivnÃ­ learning rate** s exponenciÃ¡lnÃ­m decay
- âœ… **EvaluaÄnÃ­ reÅ¾im** - mÄ›Å™enÃ­ bez explorace
- âœ… **TensorBoard logging** - real-time monitoring
- âœ… **Automatic checkpointing** - uklÃ¡dÃ¡nÃ­ nejlepÅ¡Ã­ch modelÅ¯

---

## ğŸ—ï¸ Architektura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DQN AGENT ARCHITECTURE                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  Environment  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Policy Networkâ”‚             â”‚
â”‚  â”‚ (LunarLander) â”‚         â”‚  (Dueling DQN) â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚         â”‚                           â”‚                      â”‚
â”‚         â”‚ state, reward, done       â”‚ Q-values            â”‚
â”‚         â–¼                           â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚     Prioritized Replay Buffer           â”‚              â”‚
â”‚  â”‚  (stores experiences with priorities)   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚         â”‚                                                  â”‚
â”‚         â”‚ sample batch (prioritized)                      â”‚
â”‚         â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Policy Network â”‚         â”‚ Target Network â”‚           â”‚
â”‚  â”‚  (trainable)   â”‚         â”‚   (frozen)     â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚         â”‚                           â”‚                      â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                   â”‚                                        â”‚
â”‚                   â–¼                                        â”‚
â”‚         Double DQN Loss + TD-error                        â”‚
â”‚                   â”‚                                        â”‚
â”‚                   â–¼                                        â”‚
â”‚            Update priorities                              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dueling DQN architektura

```
Input State (8 features)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Shared Features â”‚  â† 256â†’256 neurons
â”‚   (2 layers)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Value  â”‚ â”‚ Advantage  â”‚
â”‚ Stream â”‚ â”‚  Stream    â”‚
â”‚ V(s)   â”‚ â”‚ A(s,a)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚         â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â–¼
    Q(s,a) = V(s) + (A(s,a) - mean(A))
```

---

## ğŸ“ Soubory a jejich funkce

### 1ï¸âƒ£ `train.py` - HlavnÃ­ trÃ©ninkovÃ¡ smyÄka

**ÃšÄel:** KompletnÃ­ pipeline pro trÃ©novÃ¡nÃ­ DQN agenta.

**KlÃ­ÄovÃ© komponenty:**
- `DuelingQNetwork` - neuronovÃ¡ sÃ­Å¥ pro aproximaci Q-funkcÃ­
- `PrioritizedReplayBuffer` - pamÄ›Å¥ pro uklÃ¡dÃ¡nÃ­ zkuÅ¡enostÃ­
- `compute_double_dqn_loss()` - vÃ½poÄet Double DQN loss
- `evaluate()` - evaluace bez explorace
- `train()` - hlavnÃ­ trÃ©ninkovÃ¡ smyÄka

**Vstup:**
```python
# Command line argumenty
--episodes 15000          # PoÄet epizod
--lr 1e-4                # Learning rate
--gamma 0.99             # Discount factor
--target_update 500      # Frekvence target network update
--eval_every 50          # Jak Äasto evaluovat
# ... dalÅ¡Ã­ parametry
```

**VÃ½stup:**
- Checkpointy: `{save_prefix}_ep{N}.pth`
- Best model: `{save_prefix}_best_eval{score}.pth`
- Final model: `{save_prefix}_final.pth`
- TensorBoard logy v `{logdir}/`

**UklÃ¡danÃ¡ data v checkpointu:**
```python
{
    "policy_state_dict": ...,    # VÃ¡hy policy sÃ­tÄ›
    "target_state_dict": ...,    # VÃ¡hy target sÃ­tÄ›
    "optimizer_state_dict": ..., # Stav optimizeru
    "scheduler_state_dict": ..., # Stav LR scheduleru (pokud aktivnÃ­)
    "ep": 1000,                  # ÄŒÃ­slo epizody
    "total_steps": 250000,       # CelkovÃ½ poÄet krokÅ¯
    "avg100": 267.5,             # PrÅ¯mÄ›r poslednÃ­ch 100 epizod
    "best_eval": 285.3           # NejlepÅ¡Ã­ eval reward
}
```

---

### 2ï¸âƒ£ `visualize.py` - Vizualizace natrÃ©novanÃ©ho agenta

**ÃšÄel:** SpuÅ¡tÄ›nÃ­ a vizualizace policy natrÃ©novanÃ©ho agenta.

**KlÃ­ÄovÃ© funkce:**
- `DuelingQNetwork` - stejnÃ¡ architektura jako v train.py
- `evaluate()` - spuÅ¡tÄ›nÃ­ epizod s/bez renderovÃ¡nÃ­
- DetailnÃ­ statistiky (reward, dÃ©lka, akce)
- Detekce ÃºspÄ›Å¡nÃ½ch pÅ™istÃ¡nÃ­

**PouÅ¾itÃ­:**
```bash
# ZÃ¡kladnÃ­ vizualizace
python visualize.py --model checkpoints/dqn_lunar_best.pth --episodes 5 --render

# S detailnÃ­m vÃ½pisem a zpoÅ¾dÄ›nÃ­m
python visualize.py --model checkpoints/dqn_lunar_ep5000.pth \
    --episodes 10 --render --delay 0.02

# Bez renderovÃ¡nÃ­ (jen statistiky)
python visualize.py --model checkpoints/dqn_lunar_final.pth --episodes 50

# TichÃ½ reÅ¾im
python visualize.py --model checkpoints/dqn_lunar_best.pth \
    --episodes 20 --render --quiet
```

**VÃ½stup:**
```
============================================================
Episode 1/5
============================================================
  Step   1 | Action: Fire main      | Q-values: [2.45, 3.21, 5.67, 2.89]
  Step   2 | Action: Do nothing     | Q-values: [3.12, 2.56, 4.23, 3.01]
  ...

âœ… SUCCESS | Total reward:  245.67 | Steps:  89
  Action distribution: {'Do nothing': 12, 'Fire left': 25, ...}

============================================================
FINAL STATISTICS
============================================================
Episodes:          5
Successful lands:  4/5 (80.0%)
Average reward:     198.45 Â± 67.32
Min reward:          45.23
Max reward:         267.89
Median reward:      215.67
Average length:      92.4 steps
============================================================
```

---

### 3ï¸âƒ£ `eval_stats.py` - Evaluace vÅ¡ech checkpointÅ¯

**ÃšÄel:** SystematickÃ© vyhodnocenÃ­ vÅ¡ech uloÅ¾enÃ½ch checkpointÅ¯ a vizualizace progressu trÃ©ninku.

**Funkce:**
- NaÄte vÅ¡echny checkpointy ze sloÅ¾ky
- Evaluuje kaÅ¾dÃ½ model N epizodami
- Generuje CSV soubor se statistikami
- VytvoÅ™Ã­ grafy s learning curves

**PouÅ¾itÃ­:**
```bash
# ZÃ¡kladnÃ­ spuÅ¡tÄ›nÃ­ (edituj checkpoint_folder v kÃ³du)
python eval_stats.py

# VÃ½stup: eval_stats.csv + eval_stats.png
```

**Struktura CSV vÃ½stupu:**
```csv
Checkpoint,Episode,AvgReward,StdReward,MinReward,MaxReward,MedianReward,AvgLength
dqn_lunar_ep500.pth,500,180.45,45.23,-20.15,245.67,185.32,125.4
dqn_lunar_ep1000.pth,1000,235.67,38.91,120.45,298.23,240.15,98.7
...
```

**GenerovanÃ© grafy:**
1. **Training Performance** - reward s error bars + rolling average
2. **Reward Range** - min/max envelope

---

### 4ï¸âƒ£ `run_optimized_training.py` - Helper pro spuÅ¡tÄ›nÃ­ trÃ©ninku

**ÃšÄel:** ZjednoduÅ¡enÃ© spouÅ¡tÄ›nÃ­ trÃ©ninku s pÅ™edpÅ™ipravenÃ½mi konfiguracemi.

**Konfigurace:**
- `OPTIMIZED_CONFIG` - doporuÄenÃ¡ (15k epizod)
- `FAST_TEST_CONFIG` - rychlÃ½ test (1k epizod)
- `AGGRESSIVE_CONFIG` - maximÃ¡lnÃ­ vÃ½kon (20k epizod)

**PouÅ¾itÃ­:**
```bash
python run_optimized_training.py

# InteraktivnÃ­ vÃ½bÄ›r:
# 1. OPTIMIZED (doporuÄeno)
# 2. FAST_TEST
# 3. AGGRESSIVE
# 4. CUSTOM
```

---

## ğŸ› ï¸ Instalace

### Krok 1: VytvoÅ™ virtuÃ¡lnÃ­ prostÅ™edÃ­
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

### Krok 2: Instaluj zÃ¡vislosti
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install gymnasium[box2d]
pip install numpy matplotlib tensorboard tqdm
```

### Krok 3: OvÄ›Å™ instalaci
```bash
python -c "import gymnasium; import torch; print('OK')"
```

---

## ğŸ® PouÅ¾itÃ­

### 1. ZÃ¡kladnÃ­ trÃ©nink
```bash
# RychlÃ½ test (100 epizod, ~10 minut)
python train.py --episodes 100 --eval_every 20

# ZÃ¡kladnÃ­ trÃ©nink (2000 epizod, ~2-3 hodiny)
python train.py

# PlnÃ½ trÃ©nink (15000 epizod, ~12-15 hodin)
python train.py --episodes 15000 --target_update 500 --lr_decay
```

### 2. OptimalizovanÃ½ trÃ©nink
```bash
# PomocÃ­ helper skriptu
python run_optimized_training.py
# â†’ Vyber moÅ¾nost 1 (OPTIMIZED)

# Nebo pÅ™Ã­mo:
python train.py \
    --episodes 15000 \
    --target_update 500 \
    --lr_decay \
    --lr_gamma 0.9999 \
    --scheduler_step_every 100 \
    --eps_end 0.01 \
    --eps_decay_steps 120000 \
    --eval_every 50 \
    --eval_episodes 20 \
    --save_prefix checkpoints_optimized/dqn_lunar \
    --logdir runs/lunar_dqn_optimized
```

### 3. Monitoring bÄ›hem trÃ©ninku

**TensorBoard:**
```bash
# V novÃ©m terminÃ¡lu
tensorboard --logdir runs/lunar_dqn_optimized

# OtevÅ™i: http://localhost:6006
```

**SledovanÃ© metriky:**
- `train/episode_reward` - reward kaÅ¾dÃ© epizody
- `train/avg_reward_100` - klouzavÃ½ prÅ¯mÄ›r (main metric!)
- `train/avg_loss` - prÅ¯mÄ›rnÃ¡ loss
- `train/avg_q_value` - prÅ¯mÄ›rnÃ© Q-values
- `train/epsilon` - aktuÃ¡lnÃ­ Îµ (explorace)
- `train/episode_length` - dÃ©lka epizod
- `train/learning_rate` - aktuÃ¡lnÃ­ LR
- `eval/reward_mean` - evaluaÄnÃ­ reward (bez explorace)

### 4. Vizualizace natrÃ©novanÃ©ho agenta

```bash
# Najdi best model
ls checkpoints_optimized/dqn_lunar_best_*.pth

# Zobraz 10 her
python visualize.py \
    --model checkpoints_optimized/dqn_lunar_best_eval285.pth \
    --episodes 10 \
    --render

# S detailnÃ­m vÃ½pisem
python visualize.py \
    --model checkpoints_optimized/dqn_lunar_best_eval285.pth \
    --episodes 5 \
    --render \
    --delay 0.02
```

### 5. Evaluace vÅ¡ech checkpointÅ¯

```bash
# 1. Uprav checkpoint_folder v eval_stats.py:
# checkpoint_folder = "checkpoints_optimized"

# 2. SpusÅ¥ evaluaci
python eval_stats.py

# 3. VÃ½sledky:
# - eval_stats.csv (tabulka)
# - eval_stats.png (grafy)
```

---

## ğŸ”¬ PokroÄilÃ© funkce

### Resume trÃ©ninku z checkpointu

**Postup:**
1. Najdi checkpoint: `checkpoints/dqn_lunar_ep5000.pth`
2. Uprav `train.py` - pÅ™idej funkci `load_checkpoint()`
3. Nebo zkopÃ­ruj model a pokraÄuj s jinÃ½m `save_prefix`

```python
# PÅ™idat do train.py pÅ™ed hlavnÃ­ smyÄku:
if args.resume:
    checkpoint = torch.load(args.resume)
    policy_net.load_state_dict(checkpoint["policy_state_dict"])
    target_net.load_state_dict(checkpoint["target_state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
    start_episode = checkpoint["ep"] + 1
    total_steps = checkpoint["total_steps"]
    print(f"Resumed from episode {start_episode}")
```

### Experimenty s hyperparametry

**Learning rate:**
```bash
# NiÅ¾Å¡Ã­ LR (konzervativnÄ›jÅ¡Ã­)
python train.py --lr 5e-5 --episodes 20000

# VyÅ¡Å¡Ã­ LR (rychlejÅ¡Ã­, ale rizikovÄ›jÅ¡Ã­)
python train.py --lr 2e-4 --episodes 10000
```

**Target network update:**
```bash
# ÄŒastÄ›jÅ¡Ã­ update (rychlejÅ¡Ã­ adaptace)
python train.py --target_update 250

# MÃ©nÄ› ÄastÃ© (stabilnÄ›jÅ¡Ã­)
python train.py --target_update 2000
```

**Epsilon decay:**
```bash
# RychlejÅ¡Ã­ decay (dÅ™Ã­vÄ›jÅ¡Ã­ exploitace)
python train.py --eps_decay_steps 50000

# PomalejÅ¡Ã­ decay (delÅ¡Ã­ explorace)
python train.py --eps_decay_steps 200000
```

**VÄ›tÅ¡Ã­ sÃ­Å¥:**
```bash
python train.py --hidden 512 --lr 5e-5
```

### SrovnÃ¡nÃ­ bÄ›hÅ¯ v TensorBoard

```bash
# SpusÅ¥ vÃ­ce trÃ©ninkÅ¯ s rÅ¯znÃ½mi logdiry:
python train.py --logdir runs/run1_baseline
python train.py --lr 5e-5 --logdir runs/run2_lower_lr
python train.py --target_update 250 --logdir runs/run3_fast_target

# Zobraz vÅ¡echny najednou:
tensorboard --logdir runs
```

---

## âš™ï¸ Hyperparametry

### KritickÃ© parametry (nejvÃ­ce ovlivÅˆujÃ­ vÃ½kon)

| Parametr | Default | DoporuÄeno | Vliv |
|----------|---------|------------|------|
| `--lr` | 1e-4 | 1e-4 aÅ¾ 5e-5 | Rychlost uÄenÃ­ |
| `--target_update` | 1000 | 500-1000 | Stabilita Q-values |
| `--eps_decay_steps` | 100k | 100k-150k | DÃ©lka explorace |
| `--gamma` | 0.99 | 0.99 | DiskontovÃ¡nÃ­ |

### Explorace

| Parametr | Default | Popis |
|----------|---------|-------|
| `--eps_start` | 1.0 | PoÄÃ¡teÄnÃ­ Îµ (100% nÃ¡hodnÃ© akce) |
| `--eps_end` | 0.05 | FinÃ¡lnÃ­ Îµ (5% nÃ¡hodnÃ© akce) |
| `--eps_decay_steps` | 100k | KrokÅ¯ do plnÃ© exploitace |

**DoporuÄenÃ­:**
- Pro sloÅ¾itÃ© prostÅ™edÃ­: `--eps_end 0.01` (mÃ©nÄ› nÃ¡hody)
- Pro jednoduchÃ©: `--eps_end 0.1` (vÃ­c explorace)

### Replay buffer

| Parametr | Default | Popis |
|----------|---------|-------|
| `--buffer_size` | 200k | Kapacita bufferu |
| `--batch_size` | 128 | Velikost batch |
| `--per_alpha` | 0.6 | Prioritizace (0=uniform, 1=full) |
| `--per_beta_start` | 0.4 | Importance sampling korekce |

### Learning rate scheduling

| Parametr | Default | Popis |
|----------|---------|-------|
| `--lr_decay` | False | Aktivace decay |
| `--lr_gamma` | 0.9999 | MultiplikÃ¡tor na step |
| `--scheduler_step_every` | 100 | KrokÅ¯ mezi updaty |

**Formula:** `new_lr = current_lr * (gamma ** steps)`

### SÃ­Å¥

| Parametr | Default | Popis |
|----------|---------|-------|
| `--hidden` | 256 | Neurons v hidden layers |

**Varianty:**
- MalÃ¡: `--hidden 128` (rychlÃ©, mÃ©nÄ› kapacity)
- Default: `--hidden 256` (vyvÃ¡Å¾enÃ©)
- VelkÃ¡: `--hidden 512` (pomalÃ©, vÃ­ce kapacity)

---

## ğŸ› Troubleshooting

### ProblÃ©m 1: Loss exploduje (NaN)
**PÅ™Ã­znaky:** Loss skoÄÃ­ na `inf` nebo `nan`

**Å˜eÅ¡enÃ­:**
```bash
# SnÃ­Å¾it learning rate
python train.py --lr 5e-5

# ZpÅ™Ã­snit gradient clipping
python train.py --max_grad_norm 1.0

# ÄŒastÄ›jÅ¡Ã­ target update
python train.py --target_update 500
```

### ProblÃ©m 2: Agent se nenauÄÃ­ (reward stagnuje)
**PÅ™Ã­znaky:** Avg100 zÅ¯stÃ¡vÃ¡ pod 0 po 1000+ epizodÃ¡ch

**Å˜eÅ¡enÃ­:**
```bash
# DelÅ¡Ã­ explorace
python train.py --eps_decay_steps 200000

# VÄ›tÅ¡Ã­ buffer
python train.py --buffer_size 500000

# VyÅ¡Å¡Ã­ learning rate
python train.py --lr 2e-4
```

### ProblÃ©m 3: NestabilnÃ­ uÄenÃ­ (velkÃ© vÃ½kyvy)
**PÅ™Ã­znaky:** Reward skÃ¡Äe nahoru/dolÅ¯ (+200 â†’ -50 â†’ +150)

**Å˜eÅ¡enÃ­:**
```bash
# Aktivovat LR decay
python train.py --lr_decay --lr_gamma 0.9999

# MÃ©nÄ› ÄastÃ© target update
python train.py --target_update 2000

# MenÅ¡Ã­ batch
python train.py --batch_size 64
```

### ProblÃ©m 4: Catastrophic forgetting
**PÅ™Ã­znaky:** Po dobrÃ©m vÃ½konu (250+) nÃ¡hle pokles (<100)

**Å˜eÅ¡enÃ­:**
```bash
# SnÃ­Å¾it epsilon end
python train.py --eps_end 0.01

# Aktivovat LR decay
python train.py --lr_decay

# MenÅ¡Ã­ LR v pozdnÃ­ch fÃ¡zÃ­ch
python train.py --lr 5e-5 --lr_decay --lr_gamma 0.99999
```

### ProblÃ©m 5: TensorBoard neukazuje data
**Å˜eÅ¡enÃ­:**
```bash
# Zkontroluj sloÅ¾ku
ls runs/lunar_dqn_optimized

# Restartuj TensorBoard
# CTRL+C a znovu:
tensorboard --logdir runs/lunar_dqn_optimized --reload_interval 5
```

### ProblÃ©m 6: CUDA out of memory
**Å˜eÅ¡enÃ­:**
```bash
# MenÅ¡Ã­ batch
python train.py --batch_size 64

# Nebo pouÅ¾ij CPU
python train.py --force_cpu
```

### ProblÃ©m 7: Visualize.py nefunguje
**Chyba:** `RuntimeError: Error(s) in loading state_dict`

**Å˜eÅ¡enÃ­:**
```python
# Zkontroluj Å¾e DuelingQNetwork v visualize.py
# odpovÃ­dÃ¡ pÅ™esnÄ› tÃ© v train.py (stejnÃ© hidden layers)
```

---

## ğŸ“Š OÄekÃ¡vanÃ© vÃ½sledky

### Timeline uÄenÃ­ (optimalizovanÃ¡ konfigurace)

| Epizoda | OÄekÃ¡vanÃ½ Avg100 | Popis |
|---------|------------------|-------|
| 0-500 | -100 â†’ 0 | Agent se uÄÃ­ zÃ¡klady |
| 500-1500 | 0 â†’ 150 | PostupnÃ© zlepÅ¡ovÃ¡nÃ­ |
| 1500-3000 | 150 â†’ 200 | DosaÅ¾enÃ­ target |
| 3000-8000 | 200 â†’ 260 | Stabilizace |
| 8000-15000 | 260 â†’ 280+ | Fine-tuning |

### KvalitativnÃ­ milnÃ­ky

**Episode ~300:** Agent pÅ™estÃ¡vÃ¡ okamÅ¾itÄ› havarovat
**Episode ~800:** PrvnÃ­ ÃºspÄ›Å¡nÃ¡ pÅ™istÃ¡nÃ­
**Episode ~1500:** KonzistentnÄ› mezi vlajkami
**Episode ~3000:** HladkÃ© pÅ™istÃ¡vÃ¡nÃ­ s kontrolou
**Episode ~8000+:** Near-optimal policy

---

## ğŸ“š Reference a zdroje

### Papery
- [DQN (Mnih et al., 2015)](https://www.nature.com/articles/nature14236)
- [Double DQN (van Hasselt et al., 2015)](https://arxiv.org/abs/1509.06461)
- [Dueling DQN (Wang et al., 2016)](https://arxiv.org/abs/1511.06581)
- [Prioritized Experience Replay (Schaul et al., 2015)](https://arxiv.org/abs/1511.05952)

### Dokumentace
- [Gymnasium](https://gymnasium.farama.org/)
- [PyTorch](https://pytorch.org/docs/)
- [TensorBoard](https://www.tensorflow.org/tensorboard)

### ProstÅ™edÃ­
- [LunarLander-v3](https://gymnasium.farama.org/environments/box2d/lunar_lander/)

---

## ğŸ¤ Contributing

Pro vylepÅ¡enÃ­ nebo reporting bugÅ¯:
1. Experimentuj s hyperparametry
2. Zaznamenej vÃ½sledky do TensorBoard
3. Zdokumentuj zmÄ›ny

---

## ğŸ“ Licence

Tento projekt je vytvoÅ™en pro vzdÄ›lÃ¡vacÃ­ ÃºÄely.

---

**Verze dokumentace:** 1.0  
**Datum:** 2025  
**Kompatibilita:** Python 3.8+, PyTorch 2.0+, Gymnasium 0.29+

---

## ğŸ“ Tips & Tricks

### 1. RychlÃ© testovÃ¡nÃ­ zmÄ›n
```bash
# VÅ¾dy nejprve otestuj na 100 epizodÃ¡ch
python train.py --episodes 100 --save_prefix test/dqn --logdir runs/test
```

### 2. SrovnÃ¡nÃ­ s baseline
```bash
# VÅ¾dy mÄ›j baseline bÄ›h pro srovnÃ¡nÃ­
python train.py --logdir runs/baseline --seed 42
python train.py --lr 5e-5 --logdir runs/experiment1 --seed 42
```

### 3. Grid search hyperparametrÅ¯
```bash
for lr in 1e-4 5e-5 2e-4; do
    python train.py --lr $lr --logdir runs/lr_$lr --seed 42
done
```

### 4. SledovÃ¡nÃ­ GPU
```bash
# V novÃ©m terminÃ¡lu
watch -n 1 nvidia-smi
```

### 5. Background trÃ©nink
```bash
# Linux/Mac
nohup python train.py --episodes 15000 > training.log 2>&1 &

# SledovÃ¡nÃ­ logu
tail -f training.log
```

---

**HodnÄ› Å¡tÄ›stÃ­ s trÃ©ninkem! ğŸš€ğŸŒ™**